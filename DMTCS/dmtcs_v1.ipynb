{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b35070-32fd-41c6-aff3-be14a4fd29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from google.cloud import storage, vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import fitz\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"gcloud_key.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0ca5c4c-a2c9-4fea-8d8e-0386806d5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    return f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "\n",
    "def delete_from_gcs(bucket_name, blob_name):\n",
    "    \"\"\"Deletes a file from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()\n",
    "\n",
    "    print(f\"Blob {blob_name} deleted.\")\n",
    "\n",
    "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a file from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(f\"Blob {source_blob_name} downloaded to {destination_file_name}.\")\n",
    "\n",
    "def detect_text_in_pdf(bucket_name, gcs_input_uri, gcs_output_uri, local_output_folder, batch_size = 1):\n",
    "    client = vision_v1.ImageAnnotatorClient()\n",
    "\n",
    "    # Configure the request for PDF/TIFF processing\n",
    "    input_config = vision_v1.InputConfig(\n",
    "        gcs_source=vision_v1.GcsSource(uri=gcs_input_uri),\n",
    "        mime_type='application/pdf'  # or 'image/tiff' for TIFF files\n",
    "    )\n",
    "\n",
    "    output_config = vision_v1.OutputConfig(\n",
    "        gcs_destination=vision_v1.GcsDestination(uri=gcs_output_uri),\n",
    "        batch_size=batch_size  # Specifies how many pages to process at once\n",
    "    )\n",
    "\n",
    "    # Create the request\n",
    "    async_request = vision_v1.AsyncAnnotateFileRequest(\n",
    "        features=[vision_v1.Feature(type=vision_v1.Feature.Type.DOCUMENT_TEXT_DETECTION)],\n",
    "        input_config=input_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    # Perform the request\n",
    "    operation = client.async_batch_annotate_files(requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish...')\n",
    "    response = operation.result(timeout=300)\n",
    "\n",
    "    # Ensure the local output folder exists\n",
    "    if not os.path.exists(local_output_folder):\n",
    "        os.makedirs(local_output_folder)\n",
    "\n",
    "    # Download all output files from GCS\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=\"vision/\")\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(\"vision/output-\") and blob.name.endswith(\".json\"):\n",
    "            local_output_file = os.path.join(local_output_folder, os.path.basename(blob.name))\n",
    "            try:\n",
    "                download_from_gcs(bucket_name, blob.name, local_output_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading the output file {blob.name}: {e}\")\n",
    "\n",
    "            # Delete the result file from GCS\n",
    "            delete_from_gcs(bucket_name, blob.name)\n",
    "\n",
    "    print(f'Results saved to {local_output_folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a6f76b8-72ac-48a0-a4a7-de8137f5837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob vision/output-1-to-1.json deleted.\n"
     ]
    }
   ],
   "source": [
    "delete_from_gcs(bucket_name, f\"vision/output-1-to-1.json\") #this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "62d5065b-b373-46a3-83be-56755ef21eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN THE NEXT BLOCK IF YOU ARE SURE YOU WANT TO RUN 13 PAGES?\n",
      "KEEP IN MIND 1000 PAGES COSTS AROUND $1.50+$0.005*1000/5 = $2.5 (upper bound)\n"
     ]
    }
   ],
   "source": [
    "source_file_name = 'test1.pdf'\n",
    "\n",
    "def get_number_of_pages(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    number_of_pages = document.page_count\n",
    "    document.close()\n",
    "    return number_of_pages\n",
    "num_pages = get_number_of_pages(source_file_name)\n",
    "print(f\"RUN THE NEXT BLOCK IF YOU ARE SURE YOU WANT TO RUN {num_pages} PAGES?\")\n",
    "print(\"KEEP IN MIND 1000 PAGES COSTS AROUND $1.50+$0.005*1000/5 = $2.5 (upper bound)\") \n",
    "#i didn't put much effort into estimating $2.5. But I was super conservative with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da134985-1a6d-4b65-b49e-e21563ea8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File test1.pdf uploaded to uploaded_file.pdf.\n",
      "Waiting for the operation to finish...\n",
      "Blob vision/output-1-to-7.json downloaded to outputs/output-1-to-7.json.\n",
      "Blob vision/output-1-to-7.json deleted.\n",
      "Blob vision/output-8-to-13.json downloaded to outputs/output-8-to-13.json.\n",
      "Blob vision/output-8-to-13.json deleted.\n",
      "Results saved to outputs\n",
      "slowest operation:  36.5544331073761\n",
      "Blob uploaded_file.pdf deleted.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'sri-cloud-vision-api-bucket'\n",
    "destination_blob_name = 'uploaded_file.pdf'\n",
    "local_output_folder = 'outputs'\n",
    "gcs_output_uri = f\"gs://{bucket_name}/vision/\"\n",
    "batch_size = 7\n",
    "\n",
    "gcs_input_uri = upload_to_gcs(bucket_name, source_file_name, destination_blob_name) #this works\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok = True)\n",
    "\n",
    "s = time.time()\n",
    "detect_text_in_pdf(bucket_name, gcs_input_uri, gcs_output_uri, local_output_folder, batch_size)\n",
    "e = time.time()\n",
    "print(\"slowest operation: \", e-s)\n",
    "delete_from_gcs(bucket_name, destination_blob_name) #this works\n",
    "\n",
    "#I mean like 3 sec per page? it's whatever ig. batch it by min((n+1)/2, 10)...\n",
    "#the gpt4o call is fast af, so overall it's still DMTOCS_pro speed. prob slightly faster if i batch nondegenerately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28cfec66-9d0d-4595-b301-b4e2b908dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = list(set(os.listdir(\"outputs\"))-{\".DS_Store\"})\n",
    "tl = [(int(file.split('-')[1]), file) for file in filelist]\n",
    "tl.sort()\n",
    "ptd = {}\n",
    "cnter = 0\n",
    "for file in tl:\n",
    "    with open(os.path.join(\"outputs\", file[1]), 'r') as file: \n",
    "    \ttd = json.load(file)\n",
    "    for idx in range(len(td[\"responses\"])):\n",
    "        assert(td[\"responses\"][idx][\"context\"][\"pageNumber\"] == cnter+1)\n",
    "        cnter+=1\n",
    "        ptd[cnter] = td[\"responses\"][idx][\"fullTextAnnotation\"][\"text\"]\n",
    "with open(\"formatted_outputs.json\", 'w') as file: \n",
    "\tjson.dump(ptd, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8f325-12e3-42f4-969f-140ada8e884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now I plug context into gpt4o. to get it to do multimodal.\n",
    "#also i should probably look at token estimates before I do this... as well as page estimates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b279d5da-cdf3-43c4-981d-42ce2a926c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i can select however, but ig i'll for example i'll do it on num pages:\n",
    "pages = [1,2,3,4,5,6]\n",
    "full_text = \"\"\n",
    "for page in pages:\n",
    "  full_text+=(ptd[page]+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8f4351f5-4603-4391-9c6d-5b107fe453b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTIMATED NUMBER OF TOKENS: 7422\n",
      "THIS COSTS $0.150 PER 1 MILLION TOKENS. BEWARNED\n"
     ]
    }
   ],
   "source": [
    "len_estimate = len(full_text.split())\n",
    "print(f\"ESTIMATED NUMBER OF TOKENS: {2*len_estimate}\")\n",
    "print(\"THIS COSTS $0.150 PER 1 MILLION TOKENS. BEWARNED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5815a2ff-539b-47a0-9a1d-5e55cc0bac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"open_ai_api_key\" \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "477f5f0c-3238-4b6f-a81f-66dc6a8bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-4o-mini is mad cheap... \n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": full_text},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the core concepts of this paper, and all key ideas in detail to me.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3c6e066-7cea-422d-9143-efd370fa2a66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper titled \"GINopic: Topic Modeling with Graph Isomorphism Network\" introduces a novel approach to topic modeling that integrates the principles of graph theory with advanced neural network techniques, specifically Graph Isomorphism Networks (GIN). Below are the core concepts and key ideas discussed in the paper:\n",
      "\n",
      "### 1. **Background on Topic Modeling**\n",
      "Topic modeling is a technique used to discover the underlying topics present in large collections of texts in an unsupervised manner. Traditional models, like Latent Dirichlet Allocation (LDA), assume that each document is a mixture of topics, represented as distributions over words. While effective, these earlier methods may overlook the complex dependencies between words in a document.\n",
      "\n",
      "### 2. **Motivation for GINopic**\n",
      "Recent advances in topic modeling have incorporated contextualized embeddings (e.g., BERT) to account for word semantics. However, these models typically treat documents as sequences of words and often ignore the intricate relationships between them. The authors argue that there's a need to explicitly capture these mutual dependencies to improve topic coherence and representation.\n",
      "\n",
      "### 3. **Graph Representation of Documents**\n",
      "To address the shortcomings of previous approaches, GINopic constructs document graphs where:\n",
      "- **Nodes** represent words.\n",
      "- **Edges** denote relationships between words (e.g., semantic or syntactic dependencies).\n",
      "By using this graph structure, it becomes possible to represent and analyze the correlations between words more effectively.\n",
      "\n",
      "### 4. **Graph Isomorphism Networks (GIN)**\n",
      "The authors leverage GINs because they have been shown to be powerful tools for graph representation learning, with capabilities equivalent to the Weisfeiler-Lehman (WL) graph isomorphism test. GINs excel at capturing neighborhood information in graphs, which is crucial for understanding the document's word correlation.\n",
      "\n",
      "### 5. **Components of GINopic**\n",
      "The proposed GINopic framework includes several critical components:\n",
      "- **Graph Construction**: Each document is represented as a weighted, undirected graph, where the weights of edges represent the similarity between words (calculated using cosine similarity).\n",
      "- **Document Graph Representation Learning**: The GIN processes the document graph to produce unique embeddings that capture the topological features of the graph.\n",
      "- **Variational Autoencoder (VAE) Framework**: GINopic employs a VAE for the probabilistic modeling of topics. An encoder extracts the latent representation of the document, while a decoder reconstructs the word distribution using the inferred topic structure.\n",
      "\n",
      "### 6. **Loss Function and Training Objective**\n",
      "The model is trained to maximize the Evidence Lower Bound (ELBO), which consists of:\n",
      "- **Reconstruction Loss**: Measures how well the model can reconstruct the input document from the learned latent representation.\n",
      "- **Kullback-Leibler (KL) Divergence**: Encourages the learned latent distribution to be close to a prior distribution, facilitating better topic modeling.\n",
      "\n",
      "### 7. **Experimental Evaluation**\n",
      "The authors evaluate the performance of GINopic through intrinsic (e.g., topic coherence and diversity) and extrinsic (document classification) tasks on benchmark datasets. They compare GINopic with existing topic models, such as GraphBTM and GNTM, demonstrating that GINopic achieves superior outcomes in terms of coherence, diversity, and predictive capability.\n",
      "\n",
      "### 8. **Key Contributions**\n",
      "The paper presents several contributions:\n",
      "- Introduction of GINopic as a novel neural topic model that effectively captures word correlations.\n",
      "- Comprehensive empirical evaluations that highlight its efficacy over existing models.\n",
      "- Insightful discussions of the methodology, including the choice of graph construction, sensitivity analysis, and model architecture.\n",
      "\n",
      "### 9. **Conclusion and Future Directions**\n",
      "The authors posit that GINopic represents a significant advancement in topic modeling by leveraging GIN to encode complex relationships between words, leading to more coherent and interpretable topics. They emphasize the potential for further research into optimizing the graph construction and exploring the applicability of GINs in other natural language processing tasks.\n",
      "\n",
      "In summary, GINopic integrates the strengths of graph theory with modern neural network architectures to improve topic modeling by capturing the intricate dependencies between words in documents. This approach opens avenues for better representation of topics in large text corpora.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0b08247-52ca-4bed-992d-18992ab6a4f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build **GINopic**, you will need to set up a Python environment with appropriate libraries for data manipulation, graph processing, and neural network training. Below, I will provide starter code, an outline of the architecture, and some recommendations for infrastructure requirements.\n",
      "\n",
      "### Recommended Infrastructure\n",
      "\n",
      "1. **Python Environment**: \n",
      "   - Use Python 3.7 or higher.\n",
      "   - Recommended libraries include:\n",
      "     - `numpy`\n",
      "     - `pandas`\n",
      "     - `torch` (PyTorch)\n",
      "     - `torch_geometric` (for graph neural networks)\n",
      "     - `scikit-learn` (for data processing and evaluation)\n",
      "     - `matplotlib` (for visualizations)\n",
      "     - `transformers` (for embeddings like BERT, if needed)\n",
      "\n",
      "2. **Compute Resources**: \n",
      "   - A machine with a decent GPU (e.g., NVIDIA RTX series) for training your model as GINs can be computationally intensive.\n",
      "   - At least 16 GB of RAM for handling large datasets.\n",
      "\n",
      "3. **Storage**: \n",
      "   - Sufficient storage for your datasets. SSDs are preferable for faster I/O operations.\n",
      "\n",
      "### Starter Code\n",
      "\n",
      "Below is a simplified version of the GINopic architecture, emphasizing document graph construction, GIN model setup, and a VAE structure. Note that this is a skeleton code and may require modifications to fully implement GINopic.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch_geometric\n",
      "from torch_geometric.data import Data\n",
      "from torch_geometric.nn import GINConv\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# GIN layer definition\n",
      "class GIN(nn.Module):\n",
      "    def __init__(self, num_features):\n",
      "        super(GIN, self).__init__()\n",
      "        self.conv1 = GINConv(nn.Sequential(nn.Linear(num_features, 64), nn.ReLU(), nn.Linear(64, 64)))\n",
      "        self.conv2 = GINConv(nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 64)))\n",
      "        \n",
      "    def forward(self, x, edge_index):\n",
      "        x = self.conv1(x, edge_index)\n",
      "        x = self.conv2(x, edge_index)\n",
      "        return x\n",
      "\n",
      "# Variational Autoencoder (VAE) definition\n",
      "class VAE(nn.Module):\n",
      "    def __init__(self, input_dim, hidden_dim):\n",
      "        super(VAE, self).__init__()\n",
      "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
      "        self.mu = nn.Linear(hidden_dim, hidden_dim)\n",
      "        self.logvar = nn.Linear(hidden_dim, hidden_dim)\n",
      "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
      "        \n",
      "    def reparameterize(self, mu, logvar):\n",
      "        std = torch.exp(0.5 * logvar)\n",
      "        eps = torch.randn_like(std)\n",
      "        return mu + eps * std\n",
      "    \n",
      "    def forward(self, x):\n",
      "        h = torch.relu(self.encoder(x))\n",
      "        mu = self.mu(h)\n",
      "        logvar = self.logvar(h)\n",
      "        z = self.reparameterize(mu, logvar)\n",
      "        return self.decoder(z), mu, logvar\n",
      "        \n",
      "# Document Graph Construction (simplified)\n",
      "def build_document_graph(documents):\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
      "    feature_names = vectorizer.get_feature_names_out()\n",
      "    \n",
      "    # Build a graph \n",
      "    edge_index = []\n",
      "    for i in range(tfidf_matrix.shape[0]):\n",
      "        for j in range(tfidf_matrix.shape[1]):\n",
      "            if tfidf_matrix[i, j] > 0:\n",
      "                edge_index.append((i, j))\n",
      "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
      "    \n",
      "    # Create node features\n",
      "    node_features = torch.FloatTensor(tfidf_matrix.toarray())\n",
      "    return Data(x=node_features, edge_index=edge_index)\n",
      "\n",
      "# Sample usage\n",
      "documents = [\"Text document one.\", \"Text document two.\", \"Another text document.\"]\n",
      "data = build_document_graph(documents)\n",
      "\n",
      "# Initialize models\n",
      "gin_model = GIN(num_features=data.x.size(1))\n",
      "vae_model = VAE(input_dim=data.x.size(1), hidden_dim=64)\n",
      "\n",
      "# Forward pass (example)\n",
      "gin_output = gin_model(data.x, data.edge_index)\n",
      "vae_output, mu, logvar = vae_model(gin_output)\n",
      "\n",
      "# Define a loss function and optimizer\n",
      "optimizer = torch.optim.Adam(list(gin_model.parameters()) + list(vae_model.parameters()), lr=0.001)\n",
      "\n",
      "# Example training loop\n",
      "for epoch in range(100):\n",
      "    optimizer.zero_grad()\n",
      "    loss = ...  # Define your loss function based on ELBO\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "print(\"Training complete.\")\n",
      "```\n",
      "\n",
      "### Explanation of Code\n",
      "\n",
      "- **GIN Layer**: This class defines the GIN architecture, which consists of two graph convolutional layers.\n",
      "- **VAE**: A simple VAE setup to encode and decode latent representations.\n",
      "- **Document Graph Construction**: This function constructs a graph from documents using TF-IDF as features. Edges represent relationships based on word occurrences.\n",
      "- **Training and Forward Pass**: A simple implementation that includes model initialization, forward passes, and a skeleton training loop.\n",
      "\n",
      "### Next Steps\n",
      "1. **Data Preprocessing**: Implement more sophisticated preprocessing steps for your text data, such as tokenization and normalization.\n",
      "2. **Loss Functions**: Implement the required loss function (ELBO) which combines the reconstruction loss and KL divergence.\n",
      "3. **Hyperparameter Tuning**: Experiment with different parameters such as learning rates, batch sizes, and model architectures.\n",
      "4. **Evaluation Metrics**: Include evaluation metrics to validate the model’s performance.\n",
      "5. **Advanced Features**: Explore techniques for better graph representation and handling document collections of varying lengths.\n",
      "\n",
      "This starter code is meant to give you a foundational understanding and a point to build upon for developing GINopic. You will need to expand and adapt the code to achieve the full functionality of the model as described in the paper.\n"
     ]
    }
   ],
   "source": [
    "#gpt-4o-mini is mad cheap... \n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"sysztem\", \"content\": completion.choices[0].message.content},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me how to build this. Give me some starter code using python, and and recommend infrastructure needed to build it.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
