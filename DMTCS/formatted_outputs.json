{
    "1": "arXiv:2404.02115v1 [cs.CL] 2 Apr 2024\nGINopic: Topic Modeling with Graph Isomorphism Network\nSuman Adhya and Debarshi Kumar Sanyal\nIndian Association for the Cultivation of Science, Jadavpur, Kolkata-700032, India\nadhyasuman30@gmail.com, debarshi.sanyal@iacs.res.in\nAbstract\nTopic modeling is a widely used approach for\nanalyzing and exploring large document col-\nlections. Recent research efforts have incorpo-\nrated pre-trained contextualized language mod-\nels, such as BERT embeddings, into topic mod-\neling. However, they often neglect the intrinsic\ninformational value conveyed by mutual de-\npendencies between words. In this study, we\nintroduce GINopic, a topic modeling frame-\nwork based on graph isomorphism networks\nto capture the correlation between words. By\nconducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse\nbenchmark datasets, we demonstrate the effec-\ntiveness of GINopic compared to existing topic\nmodels and highlight its potential for advancing\ntopic modeling.\nhttps://github.com/AdhyaSuman/\nGINopic\n1 Introduction\nThe rise in digital text data makes organizing them\nmanually by theme increasingly difficult. Topic\nmodeling plays a significant role here (Newman\net al., 2010; Boyd-Graber et al., 2017; Adhya and\nSanyal, 2022), as it can uncover the underlying\ntopics in documents in an unsupervised manner. In\ntopic modeling, we assume that each document is\na mixture of topics and these latent topics are also\ndefined as distribution over the words.\nMotivation: Recent approaches to neural topic\nmodeling (Bianchi et al., 2021a,b; Grootendorst,\n2022) focus on the representation of the document\nas a sequence of words, which captures the contex-\ntual information. However, words in a document\nmay be correlated to each other in a much more\ncomplex manner. So, why not explicitly consider\nthese word dependency patterns while learning the\ntopics? Several studies in the field of topic mod-\neling delve into the representation of documents\nusing graphs. In this context, nodes signify words,\nand edges depict relationships between words, such\nas syntax or semantic relations. For instance, in\nthe case of short texts, the Graph Biterm Topic\nModel (GraphBTM) (Zhu et al., 2018), an exten-\nsion of the Biterm Topic Model (BTM) (Yan et al.,\n2013), represents word co-occurrence as a graph,\nwith nodes representing words and weighted edges\nreflecting the counts of corresponding biterms. De-\nspite GraphBTM's emphasis on capturing word\ndependencies, it has been reported to exhibit poor\nperformance (Shen et al., 2021). Additionally, its\ncomputational cost escalates with an expanding vo-\ncabulary, as it constructs a single graph using the\nentire vocabulary. In contrast, the Graph Neural\nTopic Model (GNTM) (Shen et al., 2021) employs\na directed graph with word dependencies as edges\nbetween word nodes to incorporate semantic in-\nformation from words in documents. However,\nGNTM considers word dependency solely by link-\ning words within a small sliding window for a given\ndocument. This limitation makes it impossible to\naccount for word dependencies that fall outside of\nthat specific window. Furthermore, the computa-\ntional complexity of generating document graphs\nincreases with the length of the window.\nApproach: To model the mutual dependency\nbetween words while addressing the existing is-\nsues of incorporation of document graphs into topic\nmodeling, we developed a neural topic model that\ntakes the word similarity graphs for each document,\nwhere the word similarity graph is constructed us-\ning word embeddings to capture the complex corre-\nlations between the words. These document graphs\nalong with their unordered frequency-based text\nrepresentation are then used as input. We have also\nused the Graph Isomorphism Network (GIN) to\nobtain the representation for each document graph.\nWe have used GIN as it is provably the maximally\npowerful GNN under the neighborhood aggrega-\ntion framework. It is as powerful as the Weisfeiler-\nLehman graph isomorphism test (Xu et al., 2019).",
    "2": "Contributions: In summary, our work presents\nthe following key contributions:\n\u2022 We introduce GINopic, a neural topic model\nthat leverages a graph isomorphism network\nto enhance word correlations in topic model-\ning.\n\u2022 We perform a comprehensive analysis through\nquantitative, qualitative, and task-specific\nevaluations. Additionally, we visualize the\nlatent spaces generated by our model to assess\nits capability to disentangle the latent repre-\nsentations of documents.\n\u2022 We also conducted a sensitivity analysis for\nthe selection of GIN among the GNNs and\nthe choice of our graph construction method-\nology.\n2 Related Work\nTopic modeling processes extensive document col-\nlections efficiently, preserving key statistical rela-\ntionships for tasks like classification, novelty de-\ntection, summarization, and similarity judgments.\nTraditional models like Latent Dirichlet Allocation\n(LDA) (Blei et al., 2003), Probabilistic Latent Se-\nmantic Index (pLSI) (Hofmann, 2013), and Corre-\nlated Topic Model (CTM) (Lafferty and Blei, 2005)\nuse sampling-based algorithms or variational infer-\nence, but their design is limited by the need for\ncareful selection, which limits the flexibility and\nscalability of model design.\nRecent advancements in neural variational infer-\nence, particularly Auto Encoding Variational Bayes\n(AEVB) (Kingma and Welling, 2014), simplify pos-\nterior computation. Neural Variational Document\nModel (NVDM) (Miao et al., 2016) is the pioneer\nVAE-based topic model. However, following the\ntraditional topic models of applying Dirichlet-prior\nto the document-topic distribution becomes chal-\nlenging due to the limitations of the reparametriza-\ntion trick. Autoencoding Variational Inference For\nTopic Models (AVITM) (Srivastava and Sutton,\n2017) resolves this by using Laplacian approxi-\nmation of the Dirichlet parameter with Gaussian\nparameters. CombinedTM (Bianchi et al., 2021a)\nextends AVITM by incorporating sentence BERT\nembeddings alongside Bag-of-Words (BoW) rep-\nresentations. ZeroShotTM (Bianchi et al., 2021b)\nfurther extends this approach, relying solely on\nSBERT embeddings, ignoring word co-occurrence\nrelations in input documents.\nNumerous contemporary methodologies incor-\nporate Graph Neural Networks (GNNs) for topic\nmodeling. In terms of the graph construction\ntask, the Graph Biterm Topic Model (GraphBTM)\n(Zhu et al., 2018) and the Graph Neural Topic\nModel (GNTM) (Shen et al., 2021) employ a mov-\ning window-based approach with a specified win-\ndow length to model word co-occurrence relation-\nships, necessitating careful window length selec-\ntion. The graph topic model (Zhou et al., 2020) con-\nstructs document graphs based on TF-IDF scores,\ncapturing relationships with graph convolutions.\nTopic modeling with knowledge graph embedding\n(Li et al., 2019) incorporates external knowledge\ngraphs. The graph attention topic network (Yang\net al., 2020) addresses overfitting in probabilistic\nlatent semantic indexing with amortized inference\nand word embeddings. The graph relational topic\nmodel (Xie et al., 2021) explores document relation-\nships using higher-order graph attention networks.\n3 Proposed Methodology\nRecognizing the challenges in topic modeling, we\nacknowledge the necessity of capturing semantic\nsimilarity among words in a document. Addition-\nally, we note the importance of addressing the\ngraph construction issue and obtaining unique rep-\nresentations for dissimilar document graphs. In re-\nsponse to these challenges, we have introduced the\nGraph Isomorphism Network-based neural topic\nmodel, abbreviated as GINopic. The following sub-\nsections provide a detailed explanation of the graph\nconstruction methodology, model framework, and\nobjective function.\n3.1 Graph Construction\nLet D be defined as the set of all documents, V\nas the set of all words in the corpus such that\n|V| V, and E ERVXT as the word embed-\n=\ndings matrix such that its i-th row &\u2081 = RT, corre-\nsponds to the word w; E V. Now for a document\nd \u0404 D, which contains a subset of words from\nV, specifically V' words, we define its weighted\nundirected document graph Gd as the adjacency\nmatrix A\n(aij)1\u2264i,j\u2264V', where the elements a\u00bfj\n=\nare determined as follows:\naij\nif Sim(Ei, E;) < 8\nSim(E,E) otherwise\n(1)\nHere, Sim(Ei, Ei) represents the cosine similar-\nity between the word embedding vectors &; and",
    "3": "d = D\nIII\nW1\nW2\nw2v\nE = W3\nD\ncompute\ncosine-similarity\n{w, w, w,..., w}\nwd\n\"N'\n\u03c9\u03bd\nw wo wo\nwd\nA = (aij)v'xv'\n\u0e1e\u0e23\u0e48\n\u0e9e\u0eb2\nFigure 1: Graph construction methodology.\nEj. In Eq. (1), 8 is a threshold that indicates if the\nsimilarity score between two words is less than\nthen there should not be any edge between them.\nThe choice of this threshold is crucial, as opting for\na lower value makes the connections in Gd denser,\nconsequently elevating computational complexity.\nConversely, opting for a higher threshold value\nleads to a sparse document graph, a scenario also\nundesired. The optimal choice of 8 depends on\nthe type of corpus. To balance these factors, we\nconsider as a hyperparameter to be tuned.\n3.2 Model Architecture\nThe proposed model GINopic comprises a docu-\nment graph representation learning network fol-\nlowed by an encoder which is followed by a de-\ncoder. The output of the graph representation learn-\ning network is concatenated with the TF-IDF rep-\nresentation of the input document before feeding\ninto the encoder. The framework is shown in Fig.\n2 and a detailed description of these networks is\ndescribed in the following.\n3.2.1 Graph Representation Learning\nThe Weisfeiler-Lehman (WL) test serves as a\nmeans to evaluate the isomorphism of two pro-\nvided graphs. The graph representation learning\nmodule within the proposed model is designed to\nprocess document graphs as its input and produce\na unique representation for each topologically dis-\ntinct document graph, identified through the WL\ntest. To model this injective mapping we have used\nthe Graph Isomorphism Network (GIN), known for\nits equivalent expressive power to the WL graph\nkernel (Shervashidze et al., 2011). GIN is theoreti-\ncally proven as the most powerful GNN (Xu et al.,\n2019). Mathematically, the layer-wise propagation\nrule for GIN at layer 1+1 is defined as follows:\nh\n(1+1)\n= MLP (1+1) ((1 + \u20ac)h() +\nAGG ({wjih), j = N(\n(N(i)}))\n(2)\nHere, h) represents the feature vector for the\ni-th node at layer 1, N(i) denotes the set of all\nneighbors for node i, wj\u0105 signifies the edge weight\nbetween the i-th and j-th nodes.\nThe opera-\ntor AGG() stands for aggregation, and e is a\nparameter that can be learned or a fixed scalar\nvalue close to zero. Furthermore, MLP (1+1)\nrepre-\nsents the multi-layer perceptron for the (1 + 1)-th\nlayer. After applying the L number of GIN lay-\ners, the encoding of a node essentially captures its\nL-th order neighborhood's information. The de-\ntailed transformations are: [GINConv(7, H) \u2192\nBN \u2192 ReLU \u2192 [GINConv(H,H) \u2192 BN \u2192\nReLU] - GINConv (H, T') \u2192 BN], where\nGINConv(I, J) represents GIN layer with a MLP\nof input dimension I and output dimension J, H\nis the number of hidden units, BN is the batch nor-\nmalization, and ReLU is the activation function.\nThe final node embeddings of dimension 7' are\nthen summed up to obtain the representation of the\ndocument graph as follows: hG = \u03a3; h).\n3.2.2 VAE framework\nEncoder Network: The encoder network of\nGINopic, takes the combination of graph repre-\nsentation (hG \u0404 RT') and TF-IDF representation\n(xTFIDF \u0404 RV) of the input document. For this\nconcatenation, hg is first scaled to the dimen-\nsion same as of XTFIDF and then concatenated\nwith XTFIDF. Therefore, the resultant representa-\ntion is x = = CONCAT (fw (hG), XTFIDF), where\nWE RVXT' is a matrix, representing linear trans-\nformation fw: RT \u2192 RV whose weights are to\nbe learned.\nCareful selection of the prior for our model-\ning assumption is crucial. In topic modeling, the\nDirichlet distribution has been demonstrated (Wal-\nlach et al., 2009) as effective in assigning topic\nproportions for a given document. However, the\nreparametrization trick is limited to Gaussian dis-\ntributions. To integrate the Dirichlet assumption\ninto a VAE following the method proposed by (Sri-\nvastava and Sutton, 2017), we used the Laplacian",
    "4": "de D\n\u0e9e\u0eb2\nTF-IDF\nw\u2081\u2081\nGraph Representation Learning\n(L)\nDocument graph\nGIN Layers\nNode pooling\nhG\nHidden Layer\n555-3+\nEncoder\n\u03bc\nlog \u03a3\nDecoder\nFigure 2: Proposed framework for GINopic model.\napproximation to the Dir(a) distribution:\n\u03bc1 k = log ak\n\u03a31 kk\n1\n\u03b1\u03ba\n-\n1\nK\n\u03a3 log \u03b1i\n(1 - 2/2)\n2\n1\n+\nK K2\nK\ni\n\u03b1\u03af\nwhere, ai is the i-th component of the K-\ndimensional Dirichlet's parameter, \u03bc\u2081k is the k-th\ncomponent of the vector \u03bc\u2081 = RK and \u03a31 kk is\nthe k-th component of \u03a3\u2081 = RK\u00d7K, the diagonal\ncovariance matrix. Given a prior distribution and\nthe resultant input document representation vector\nx, the encoder outputs the posterior distribution\n9(z|x) = N (\u03bco, \u03a3o), where represents the\nweights of the encoder. The transformations in\nthe encoder are: [Linear(2V, H') \u2192 Softplus \u2192\n[Linear (H', H') \u2192 Softplus] L'-1\nDropout (0.2)]. This is followed by the two\nseparate and similar transformations as follows:\n[Linear (H', K) \u2192 BN] for \u03bco and \u03a30 respec-\ntively. In these expressions, V represents the\nvocabulary size, H' and L' represent the number\nof hidden units and hidden layers respectively,\nSoftplus is an activation function, and Dropout is\na regularizer.\n\u2192\nSampling Procedure: A latent representation z\nis stochastically sampled from the posterior distri-\nbution q(x) using the reparameterization trick\n(Kingma and Welling, 2014) as z = \u00b5o +\u03a31/\u00b2 \u25cf \u20ac.\nThe symbol denotes the Frobenius inner product\nand \u20ac ~ N(0, 1). The obtained latent representa-\ntion z is then used as logit to a softmax function\n\u03c3(.) in order to generate the document-topic distri-\nbution such that, 0 = \u03c3(z).\nDecoder Network: In the decoder, the topic-\nword matrix \u1e9e refers to the learnable weights of\nthe decoder network. This matrix is utilized to re-\nconstruct the word distribution \u00e2 as: \u00ee = \u03c3 (BTA)\nFollowing (Srivastava and Sutton, 2017), we re-\nlaxed the simplex constraint on \u1e9e, which is em-\npirically shown to produce better topic quality.\nThe transformations of the decoder network are,\n[Linear (K, V) \u2192 BN \u2192 Softmax], with \u03c3 em-\nployed in the output layer to generate the word\ndistribution.\n3.3 Training Objective\nThe objective function for GINopic is the same\nas ELBO which needs to be maximized in order\nto maximize the log-likelihood of the input data\ndistribution. The loss function we seek to minimize\nis defined as:\nL=LRL + LKL\n(3)\n= \u2212Ez~q4(z|x) [P\u00df(x|z)] + DKL (96(2|x)||p(z))\nIn the above expression, the first term (CRL)\nrepresents the reconstruction loss, quantified by\nthe cross-entropy between the predicted output dis-\ntribution and the input vector XTFIDF. On the\nother hand, the second term (LKL) is the Kullback-\nLeibler (KL) divergence of the learned latent space\ndistribution q(z|x) from the prior p(z) of the la-\ntent space.\n4 Experimental Settings\nWe have conducted the experiments using OCTIS\u00b9\n(Terragni et al., 2021a), a comprehensive frame-\nwork for comparing and optimizing topic models,\navailable under the MIT License.\n4.1 Datsets\nIn the experiments, we utilized five publicly avail-\nable datasets. Among these, 20NewsGroups\n(20NG) and BBC News (BBC) (Greene and Cun-\nningham, 2006) datasets were already included in\n\u00b9https://github.com/MIND-Lab/OCTIS",
    "5": "Dataset\n20NG\nBBC\nSS\nBio\nSO\n#Total #Tr #Ts/Va\nDocs Docs Docs\n16309 11415 2447\n2225 1557 334\n12270 8588 1841\n18686 13080 2803\n15696 10986 2355\nAvg. Doc.\nHyperpramerts\n20NG\nBBC SS Bio SO\nLabels\nlength\n48.020\n20\nGraph construction threshold (8):\nDim. of input node feature (7):\n#GIN layers (L):\n0.4\n0.3 0.2\n0.05 0.1\n2048\n256\n1024\n1024 64\n2\n3\n2\n2\n2\n120.116\n13.104\n7.022\n5\n#Hidden layers in MLP:\n1\n1\n1\n1\n1\n8\nDim. of Hidden layers in MLP:\n200\n50\n50\n200 300\n20\nDim. of output node feature (7'):\n768\n512 256 256 512\n5.106\n20\nTable 1: Statistics of the used datasets.\nTable 2: Value of the hyperparameters of GINopic for\neach dataset.\nOCTIS in pre-processed formats. Additionally, we\nincorporated the SearchSnippets (SS), Biomedicine\n(Bio), and StackOverflow (SO) datasets (Qiang\net al., 2022) and pre-processed them. A detailed\ndescription of these datasets is mentioned in Ap-\npendix A.1 and the pre-processing steps are men-\ntioned in Appendix A.2. Statistical descriptions of\nthese datasets can be found in Table 1. Each of\nthese corpora was divided into training, validation,\nand testing sets, with a distribution ratio of 70% for\ntraining, 15% for validation, and 15% for testing,\nwhere the training part is used to train the model,\nthe validation part is only used for the GNTM to\nmodify the learning rate accordingly and the test\npart is used to conduct the extrinsic evaluation of\nthe models.\n4.2 Baselines\nWe conducted a comparative analysis of the pro-\nposed model GINopic with the graph-based topic\nmodels, namely GraphBTM (Zhu et al., 2018)\nand GNTM (Shen et al., 2021). Unfortunately,\nfor other graph-based topic models, we could not\naccess their source code, making it impossible to\ninclude them in our comparison. Beyond the graph-\nbased models, our evaluation extended to various\nwell-known neural and traditional topic models, in-\ncluding ECRTM (Wu et al., 2023), CombinedTM\n(Bianchi et al., 2021a), ZeroShotTM (Bianchi\net al., 2021b), ProdLDA (Srivastava and Sutton,\n2017), NeuralLDA (Srivastava and Sutton, 2017),\nETM (Dieng et al., 2020), LDA (Blei et al., 2003),\nLSI (Dumais, 2004) and NMF (Zhao and Tan,\n2017). A detailed description of the configurations\nof these baselines together with their implementa-\ntion details can be found in Appendix B.\n4.3 Hyperparameter Tuning\nIn GINopic, for a given dataset the hyperparame-\nters that are tuned are mentioned in Table 2. Here,\nthe hyperparameter tuning was conducted on each\ndataset, maintaining a topic count equal to the num-\nber of labels for 50 epochs. To ensure a fair com-\nparison we have also tuned the hyperparameters for\nGNTM. However, due to computational limitations,\nwe are unable to fine-tune the hyperparameters for\nGraphBTM.\n5 Results and Discussions\nWe categorize our findings into the following sec-\ntions: (1) quantitative evaluation (Section 5.1), (2)\nextrinsic evaluation (Section 5.2), (3) qualitative\nevaluation (Section 5.4), (4) latent space visual-\nization (Section 5.3), and (5) sensitivity analysis\n(Section 5.5).\n5.1 Quantitative Evaluation\nIn the quantitative evaluation, we have evaluated\nthe topic models based on the generated topic\nquality measured by coherence and diversity met-\nrics. To measure the topic coherence we have\nused Normalized Pointwise Mutual Informa-\ntion (NPMI) (Lau et al., 2014) and Coherence\nValue (CV) (R\u00f6der et al., 2015). NPMI is com-\nmonly utilized (Adhya et al., 2022) as a surro-\ngate for human judgment of topic coherence, al-\nthough some researchers also employ CV, despite\nits known issues. We measure the diversity of top-\nics using Inverted Rank-Biased Overlap (IRBO)\n(Bianchi et al., 2021a), Word Embedding-based\nInverted Rank-Biased Overlap - Match (wI-\nM), and Word Embedding-based Inverted Rank-\nBiased Overlap - Centroid (wI-C) (Terragni et al.,\n2021b). Higher values of NPMI, CV, IRBO, WI-\nC, and wI-M indicate better performance. These\nmetrics are elaborately discussed in Appendix C.\nExperimental Setup: For a given dataset we\nrun all the models by varying the topic count in\n{20, 50, 100} U {kgold} where kgold stands for the\ngolden topic count which is the number of ground-\ntruth labels in the dataset (since they are available\nfor the datasets we used). The values of kgold for\n20NG, BBC, and M10 are 20, 5, and 8, respec-\ntively. For the robustness of the results, we have\nreported the mean value over 5 random runs for",
    "6": "NPMI\n0.10\n0.05\n20NG\nGINopic\n0.1\n0.0\nBBC\nGNTM\n0.05\n0.00\n-0.05\nCombinedTM\nSS\n0.1\n0.0\nZeroShotTM\nBio\n0.1\n0.0\n-0.1\nNMF\nSO\n-0.1\n-0.2\n0.00\n20\n50\n20NG\n100\n5\n20\n50\n100\n8\n20\n50 100\n20\n50\n100\n20\n50\n100\nBBC\nSS\nBio\nSO\n0.6\n0.4\n0.2\n0.6\n0.4\n0.2\n0.4\n0.2\n0.6\n0.4\n0.2\n0.4\n0.2\n0.0\n0.0\n0.0\n0.0\n0.0\n20\n50\n100\n5\n20\n50\n100\n8\n20\n50\n100\n20\n50\n100\n20\n50\n100\nFigure 3: Topic coherence (NPMI and CV) scores for each topic count for top-5 topic models on five datasets.\nBBC\nSS\n20NG\nBio\nSO\nModel\nNPMI CV NPMI CV\nECRTM\n-0.145 0.363 -0.041 0.625\nCombinedTM 0.086 0.617\n0.637\nZeroShotTM 0.083 0.617\n0.042\n0.024\n0.630\n0.031\nProdLDA\nNeuralLDA\nETM\nLDA\nLSI\nNMF\nGraphBTM\nGNTM\nGINopic\n0.071 0.593 0.035 0.628\n0.045 0.500 -0.065 0.472\n0.050 0.528 0.030 0.452\n0.069 0.562 0.049 0.518\n-0.019 0.400 -0.042 0.406\n0.088 0.599 0.069 0.543\n0.017 0.605 -0.173 0.484\n0.081 0.588 0.090 0.600\n0.102 0.647 0.130 0.701\nNPMI CV NPMI CV NPMI CV\n-0.388 0.474 -0.435 0.529 -0.416 0.526\n0.040 0.510 0.123 0.587 0.065 0.491\n0.504 0.120 0.579 0.056 0.486\n-0.001 0.486 0.105 0.571 0.042 0.473\n-0.114 0.400 -0.061 0.435\n-0.099 0.309 -0.136\n-0.165 0.376 -0.118 0.392\n-0.122 0.280 -0.118 0.392 -0.129 0.303\n-0.035 0.412 0.019 0.446 -0.050 0.377\n-0.322 0.444 -0.398 0.519 -0.451 0.558\n0.005 0.445 -0.039 0.400 -0.129 0.359\n0.048 0.517 0.123 0.589 0.059 0.493\n-0.177 0.407\n0.140\n-0.332 0.441\n-0.174 0.345\nTable 3: Comparison of topic models on five datasets. For each metric and each topic model, we mention the mean\nscores over topic counts {20, 50, 100} U {kgold}.\na given model, a given dataset, and a given topic\ncount.\nFindings: We present coherence scores for\nall models across datasets in Table 3. Notably,\nGINopic achieves the highest coherence scores\n(both NPMI and CV) across most datasets, except\nfor the SO dataset where it ranks second in NPMI\nscore, following CombinedTM. However, GINopic\nstill leads in CV score for the SO dataset. To pro-\nvide a comprehensive comparison, we focus on the\ntop 5 models based on their NPMI scores across\nall datasets. Figure 3 shows the mean and standard\ndeviation of NPMI and CV scores for each topic\ncount. The results establish the consistent supe-\nrior performance of GINopic compared to existing\nmodels. In terms of diversity, Table 3 displays all\nthree diversity scores. GINopic achieves the high-\nest wI-M and wI-C diversity scores across most\ndatasets, except for the 20NG dataset where its\nwI-M score is comparable to ECRTM's highest\nscore. ECRTM exhibits the highest IRBO scores\nacross all datasets due to its embedding clustering\nregularization approach, despite its poor coherence\nscores indicating ineffective topic representation\nlearning. IRBO scores of GINopic are also com-\npetitive, being close to the highest score across all\ndatasets.\n5.2 Extrinsic Evaluation\nWe have also incorporated an extrinsic task to as-\nsess the performance of the topic models, specifi-\ncally by evaluating their predictive capabilities in a\ndocument classification task.\nExperimental Setup: Our datasets include\ncategory labels for each document. We trained\nall models on the training subset of a particular\ndataset to generate kgold topics. The resulting kgold-\ndimensional document-topic vector serves as a rep-\nresentation of the document. A linear support vec-\ntor machine is then trained on these representa-\ntions, and model performance on the test subset is\nreported. We calculate the average accuracy over",
    "7": "Model\nECRTM\n20NG\nIRBO WI-M WI-C\nBBC\n0.998 0.473 0.852\nETM\nLDA\nLSI\nNMF\nGraphBTM\nGNTM\nGINopic\nSS\nSO\nIRBO WI-M WI-C IRBO WI-M wI-C IRBO WI-M WI-C IRBO WI-M WI-C\n0.999 0.454 0.848 1.000 0.442 0.839 1.000 0.433 0.838 1.000 0.382 0.825\nCombinedTM 0.988 0.468 0.895 0.978 0.442 0.888 0.993 0.45 0.888 0.983 0.443 0.887 0.985 0.392 0.878\nZeroShotTM 0.986 0.467 0.894 0.964 0.435 0.887 0.99 0.448 0.888 0.983 0.445 0.885 0.985 0.393 0.879\nProdLDA\n0.990 0.469 0.895 0.975 0.44 0.888 0.994 0.45 0.888 0.987 0.446 0.888 0.977 0.394 0.878\nNeuralLDA 0.989 0.466 0.892 0.984 0.444\n0.887 0.997 0.453 0.887 0.996 0.452 0.888 0.979 0.390 0.875\n0.802 0.37 0.87 0.802 0.354 0.874 0.647 0.294 0.867 0.344 0.138 0.843 0.490 0.187 0.842\n0.981 0.462 0.893 0.947 0.424\n0.885 0.988 0.447 0.886 0.991 0.446 0.886 0.913 0.390 0.875\n0.925 0.429 0.887 0.869 0.385 0.879 0.845 0.382 0.881 0.991 0.399 0.881 0.927 0.337 0.868\n0.975 0.458 0.892 0.966 0.432 0.886 0.978 0.443 0.887 0.988 0.443 0.887 0.984\n0.971 0.462 0.852 0.986 0.448 0.846 0.947 0.421 0.836 0.924 0.427 0.837 0.958\n0.984 0.461\n0.852 0.983 0.444 0.845 0.995 0.454 0.846 0.999 0.455 0.845 0.949 0.406 0.831\n0.989 0.468 0.895 0.992 0.457 0.893 0.998 0.454 0.889 0.983 0.462 0.888 0.986 0.497 0.879\nBio\n0.388 0.876\n0.374 0.821\nTable 4: Comparison of topic models on five datasets. For each metric and each topic model, we mention the mean\nscores over topic counts {20, 50, 100} {kgold}.\nfive runs for each dataset and present the scores in\nTable 5.\nFindings: Table 5 shows that GINopic attains\nthe highest accuracy across all datasets, except\nfor the 20NG dataset where it secures the second-\nhighest accuracy, with the GNTM closely edging\nahead.\nModel\n2018). UMAP transformed the kgold-dimensional\ndocument-topic distribution into a two-dimensional\nrepresentation, making possible to visualize.\nEach document was assigned to a cluster based\non its topic distribution vector 0, where the clus-\nter was determined by selecting the topic with the\nhighest probability. Figure 4 illustrates the clusters\nobtained for each dataset.\nETM\nLDA\nLSI\nNMF\nGraphBTM\n20NG BBC SS Bio SO\nECRTM\n0.816\n0.411\n0.492 0.361 0.457\nCombinedTM 0.397 0.796 0.706 0.493 0.715\nZeroShotTM 0.385 0.817 0.698 0.501 0.687\nProdLDA\n0.385 0.752 0.662 0.489 0.674\nNeuralLDA 0.297 0.575 0.464 0.376 0.403\n0.370 0.754 0.496 0.083 0.072\n0.428 0.798 0.440 0.364 0.412\n0.329 0.337 0.343 0.402 0.660\n0.350 0.785 0.415 0.437 0.708\n0.052 0.231 0.224 0.060 0.050\n0.449 0.806 0.222 0.049 0.053\n0.441 0.888 0.713 0.566 0.785\n20NG\n20\nBBC\nSS\n20\n10\n10\n10\n0\n0\n-10\n-10\n0\n10\n20\n0\n10\n-10\n0\n10\nBio\nSO\n15\n10\n10\n5\n0\n-5\n-10\n-10\n0\n10\n0\n10\n20\nGNTM\nGINopic\nTable 5: Average accuracy scores in the document clas-\nsification task for all the models trained with topic count\nkgold for all five datasets.\n5.3 Latent Space Visualization\nWe have further examined the latent space gener-\nated by GINopic. In topic modeling, documents\nare projected into a lower-dimensional latent (topic)\nspace.\nExperimental Setup: To visualize the latent\nspace, we have trained GINopic for the topic count\nof kgold associated with each of the five datasets.\nFollowing the training phase, we captured the\ndocument-topic distribution for each document.\nWe applied the Uniform Manifold Approximation\nand Projection (UMAP) technique, a robust di-\nmensionality reduction method (McInnes et al.,\nFigure 4: Latent space visualization for GINopic model\nacross all five datasets.\nFindings: The disentanglement of clusters is\ndepicted in Figure 4 for each dataset. Notably, the\nclarity of disentanglement is more pronounced in\nthe BBC and SS datasets compared to the other\nthree datasets 20NG, Bio, and SO. This difference\ncan be attributed to the greater challenge of dis-\nentangling 20 different labels in the 20NG, Bio,\nand SO datasets, as opposed to the BBC and SS\ndatasets with fewer distinct labels.\n5.4 Qualitative Evaluation\nSince topic models operate as unsupervised meth-\nods, it is recommended to assess their performance\nnot solely relying on automated estimates of topic\ncoherence but also through manual evaluation of",
    "8": "Model\nECRTM\nCombinedTM\nZeroShotTM\nProdLDA\nNeuralLDA\nTopics\nturkish, soviet, bullet, minority, population, burn, jewish, cold, prepare, joke\ndraft, baseball, game, shot, blue, luck, stupid, programming, basically, score\nbike, car, controller, button, camera, strategy, win, black, atheism, attribute\ngerman, publish, genocide, turkish, muslim, armenian, book, representative, european, century\nteam, hockey, season, game, draft, expansion, ticket, play, year, ice\ncar, engine, tire, bike, ride, brake, good, problem, buy, mile\ngreek, turkish, minority, genocide, state, muslim, soviet, armenian, israeli, struggle\nranger, hockey, playoff, team, game, devil, king, pen, wing, period\nmotorcycle, bike, clean, wave, ride, wheel, tip, mirror, remove, replace\narab, israeli, religious, people, religion, jewish, solution, territory, understanding, land\nyear, fund, money, spend, program, player, private, team, job, good\neat, food, car, problem, engine, brake, stone, weight, pain, day\narmy, muslim, genocide, international, turkish, village, armenian, population, organize, enter\ngoal, win, score, play, wing, penalty, playoff, team, pass, game\nfront, clean, ride, foot, bike, bar, engine, pull, weight, remove\narmenian, people, turkish, village, kill, genocide, woman, live, soldier, jewish\nETM\ngood, year, win, game, back, play, make, post, line, goal\nLDA\nLSI\nNMF\nGraphBTM\nGNTM\nGINopic\nbike, engine, mission, orbit, temperature, car, earth, space, planet, solar\nwar, jewish, israeli, land, country, arab, peace, territory, force, attack\ndouble, trade, game, hockey, final, team, star, playoff, king, regular\nbike, ride, hate, advice, bank, motorcycle, weight, good, instruction, surrender\nturkish, drive, war, armenian, russian, government, secret, military, power, jewish\nyear, car, scsi, love, bit, client, team, server, call, player\naccess, engine, power, kill, database, word, bus, attack, disk, card\nkill, woman, time, soldier, start, child, back, leave, armenian, man\npower, play, government, constitution, team, control, level, individual, idea, zone\ncar, engine, price, buy, bike, mile, ride, make, driver, tire\narmenian, afraid, neighbor, clock, soldier, turkish, floor, soviet, beat, arrive\ngame, score, car, engine, play, goal, season, playoff, shot, player\ntire, bike, connector, ide, brake, scsi, cable, car, rear, engine\nisraeli, arab, jewish, policy, land, territory, area, peace, human, population\nteam, game, play, player, win, year, good, call, point, time\ntire, oil, brake, bike, paint, weight, corner, air, lock, motorcycle\ngenocide, muslim, armenian, massacre, turkish, population, kill, government, troop, war\nteam, win, score, baseball, game, player, hockey, playoff, goal, play\ncar, bike, ride, brake, light, tire, engine, lock, side, mile\nTable 6: Some representative topics extracted from the 20NG dataset with a topic count of 100. Relevant terms\nwithin each topic are emphasized in bold.\nthe topics, as emphasized by (Hoyle et al., 2021;\nAdhya and Sanyal, 2023).\n=\nExperimental Setup: We conducted a quali-\ntative analysis of the topics, utilizing the 20NG\ndataset and training all models with the golden\ntopic count i.e. kgold 20. The results appear in\nTable 6. Note that the table exhibits aligned top-\nics, wherein the first topic listed for one model is\nsimilar to the first topic for every other model, and\nthe same goes for the rest of the topics, following\nthe alignment method proposed by (Adhya et al.,\n2023). Additionally, words closely associated with\na given topic are highlighted in bold.\nFindings: In Table 6, we showcase three top-\nics: \"Armenian genocide\", \"Sports\", and \"Au-\ntomobile\" related. Across these distinct topics,\nGINopic consistently generates more correlated\nwords compared to other models. This observation\nis supported by the consistently higher number of\nbold words for each topic in GINopic, indicating\nstronger word correlations than the other models.\n5.5 Sensitivity Analysis\n5.5.1 Choice of the Graph Neural Network\nTo empirically check the effectiveness of GIN\nover other GNNs in our model, we substitute GIN\nwith Graph Attention Network (GAT) (Veli\u010dkovi\u0107\net al., 2018), Graph SAmple and aggreGate (Graph-",
    "9": "NPMI\n0.1\n0.0\n-0.1\nGIN\nGAT\nGraphSAGE\nGCN\n0.8\n0.7\n0.6\n0.5\n0.4\n-0.2\n0.3\n20NG\nBBC\nSS\nBio\nSO\n20NG\nBBC\nSS\nBio\nSO\nFigure 5: Box plot of topic coherence (NPMI and CV) scores incorporating GIN, GAT, GraphSAGE, and GCN in\nGINopic on five datasets.\n0.15\n0.10\n0.05\n20NG\nBBC\nSS\nBio\n0.70\n0.65\n0.60\n0.55\n0.50\n0.0 0.1 0.2 0.3\n0.4\n0.5\n\u03b4\n0.0 0.1\n0.2\n\u0431\n0.3 0.4 0.5\nFigure 6: Coherence (NPMI and CV) scores for\neach dataset by varying the threshold (8) value in\n{0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}.\nSAGE) (Hamilton et al., 2017) and Graph Convo-\nlutional Network (GCN) (Kipf and Welling, 2017).\nExperimental Setup: We trained our proposed\nmodel on the five datasets, adjusting the topic count\nwithin the set {20, 50, 100} Ukgold. To ensure a fair\ncomparison, we maintained consistent parameter\nvalues across all models, aligning them with those\nof GINopic.\nFindings: In Figure 5, a box plot is pre-\nsented, illustrating the NPMI and CV scores de-\nrived from five random runs for each model across\nthe five datasets. The results indicate that the GIN-\nincorporated model consistently outperforms other\nGNN-based models across all datasets in terms of\nboth the coherence measures.\n5.5.2 Choice of the Graph Construction\nThreshold (8)\nWe have examined how the graph construction\nthreshold 8, as specified in Eq. (1), influences\nmodel performance and training time.\nExperimental Setup: Given a dataset, we have\ntrained our model for the corresponding kgold\nnumber of topics by varying the value of \u00a7 over\n{0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}. We reported the\nmean and standard deviation of the coherence\nscores (NPMI and CV) over 5 random runs in Fig-\nure 6.\nFindings: Figure 6 illustrates the optimal thresh-\nold values that maximize coherence scores (NPMI\nand CV) for each dataset. This threshold signi-\nfies that if the similarity between two nodes in a\ndocument graph falls below it, no edge connects\nthose nodes. Moreover, increasing the 8 value re-\nsults in a sparser document graph, leading to re-\nduced training time. Table 7 provides details of\nthe dataset-wise optimal threshold (8) values and\nthe corresponding percentage reductions in train-\ning time from that with the \u2642 value of 0.0. Thus,\nby tuning 8, we improve the coherence scores and\nsimultaneously reduce the training time.\nValue\nOptimal threshold (8)\nReduction (%)\n20NG\n0.4\n154.27%\nBBC SS Bio SO\n0.3\n0.2 0.05 0.1\n266.72% 16.71% 0.29% 1.06%\nTable 7: Optimal threshold (8) value along with the\npercentage of training time reduction for all five datasets.\n6 Conclusion\nWe have introduced GINopic, a neural topic model\nbased on a graph isomorphism network, and evalu-\nated its performance on five widely used datasets\nfor assessing topic models. Across the majority\nof our experiments, GINopic consistently exhibits\nsuperior topic coherence and diversity compared\nto other competitive topic models from the litera-\nture. Manual evaluation of selected topics further\nconfirms that GINopic generates more coherent\ntopics than alternative models. In extrinsic eval-\nuations, GINopic generally outperforms existing\nmodels across all the datasets, except for the 20NG\ndataset. We utilized visualizations of the latent\nspace generated by GINopic to assess its cluster-\ning disentanglement capability. Sensitivity analy-\nsis demonstrates the impact of graph construction\nthreshold values on the performance and training\ntime of GINopic. Additionally, we highlight the\neffectiveness of GIN over other graph neural net-\nworks in our topic model.",
    "10": "Limitations\nThis paper focuses solely on utilizing word simi-\nlarity for constructing document graphs. However,\nthere exist alternative methods for constructing doc-\nument graphs, such as incorporating dependency\nparse graphs. Future extensions of this work could\nexplore capturing diverse word dependencies and\nintegrating them to construct a multifaceted docu-\nment graph.\nEthics Statement\nThe topic words presented in Table 6 depict the\noutput of the topic models trained on the 20NG\ndataset. The authors have no intention to cause\nharm or offense to any community, religion, coun-\ntry, or individual.\nReferences\nSuman Adhya, Avishek Lahiri, Debarshi Kumar Sanyal,\nand Partha Pratim Das. 2022. Improving contextu-\nalized topic models with negative sampling. In Pro-\nceedings of the 19th International Conference on Nat-\nural Language Processing (ICON), pages 128\u2013138,\nNew Delhi, India. Association for Computational\nLinguistics.\nSuman Adhya, Avishek Lahiri, and Debarshi Kumar\nSanyal. 2023. Do neural topic models really need\ndropout? analysis of the effect of dropout in topic\nmodeling. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2220\u20132229, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nSuman Adhya and Debarshi Kumar Sanyal. 2022. What\ndoes the Indian Parliament discuss? an exploratory\nanalysis of the question hour in the Lok Sabha. In\nProceedings of the LREC 2022 workshop on Natural\nLanguage Processing for Political Sciences, pages 72-\n78, Marseille, France. European Language Resources\nAssociation.\nSuman Adhya and Debarshi Kumar Sanyal. 2023. Im-\nproving neural topic models with Wasserstein knowl-\nedge distillation. In Advances in Information Re-\ntrieval, pages 321-330, Cham. Springer Nature\nSwitzerland.\nFederico Bianchi, Silvia Terragni, and Dirk Hovy.\n2021a. Pre-training is a hot topic: Contextualized\ndocument embeddings improve topic coherence. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 759\u2013766,\nOnline. Association for Computational Linguistics.\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora\nNozza, and Elisabetta Fersini. 2021b. Cross-lingual\ncontextualized topic models with zero-shot learning.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1676\u20131683, Online.\nAssociation for Computational Linguistics.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent Dirichlet allocation. Journal of Ma-\nchine Learning Research, 3(Jan):993-1022.\nJordan L. Boyd-Graber, Yuening Hu, and David M.\nMimno. 2017. Applications of topic models. Found.\nTrends Inf. Retr., 11(2-3):143\u2013296.\nAdji B. Dieng, Francisco J. R. Ruiz, and David M. Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:439-453.\nSusan T. Dumais. 2004. Latent semantic analysis. An-\nnual Review of Information Science and Technology,\n38(1):188-230.\nDerek Greene and P\u00e1draig Cunningham. 2006. Practi-\ncal solutions to the problem of diagonal dominance\nin kernel document clustering. In Proceedings of the\n23rd International Conference on Machine Learn-\ning, ICML '06, page 377\u2013384, New York, NY, USA.\nAssociation for Computing Machinery.\nMaarten Grootendorst. 2022. BERTopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nThomas Hofmann. 2013. Probabilistic latent semantic\nanalysis. CoRR, abs/1301.6705.\nAlexander Hoyle, Pranav Goel, Andrew Hian-Cheong,\nDenis Peskov, Jordan Boyd-Graber, and Philip\nResnik. 2021. Is automated topic model evaluation\nbroken? the incoherence of coherence. Advances\nin Neural Information Processing Systems, 34:2018-\n2033.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational Bayes. In Proceedings of the\n2nd International Conference on Learning Represen-\ntations, ICLR 2014.\nSemi-\nThomas N. Kipf and Max Welling. 2017.\nsupervised classification with graph convolutional\nnetworks. In International Conference on Learning\nRepresentations.\nJohn Lafferty and David Blei. 2005. Correlated topic\nmodels. In Advances in Neural Information Process-\ning Systems, volume 18. MIT Press.",
    "11": "Jey Han Lau, David Newman, and Timothy Baldwin.\n2014. Machine reading tea leaves: Automatically\nevaluating topic coherence and topic model quality.\nIn Proceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 530-539, Gothenburg, Sweden.\nAssociation for Computational Linguistics.\nDingcheng Li, Siamak Zamani, Jingyuan Zhang, and\nPing Li. 2019. Integration of knowledge graph em-\nbedding into topic modeling with hierarchical Dirich-\nlet process. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n940-950, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas\nGrossberger. 2018. Umap: Uniform manifold ap-\nproximation and projection. The Journal of Open\nSource Software, 3(29):861.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-\nral variational inference for text processing. In Pro-\nceedings of The 33rd International Conference on\nMachine Learning, volume 48 of Proceedings of Ma-\nchine Learning Research, pages 1727-1736, New\nYork, New York, USA. PMLR.\nDavid Newman, Youn Noh, Edmund Talley, Sarvnaz\nKarimi, and Timothy Baldwin. 2010. Evaluating\ntopic models for digital libraries. In Proceedings\nof the 10th Annual Joint Conference on Digital Li-\nbraries, JCDL '10, page 215\u2013224, New York, NY,\nUSA. Association for Computing Machinery.\nJ. Qiang, Z. Qian, Y. Li, Y. Yuan, and X. Wu. 2022.\nShort text topic modeling techniques, applications,\nand performance: A survey. IEEE Transactions on\nKnowledge & Data Engineering, 34(03):1427\u20131445.\nMichael R\u00f6der, Andreas Both, and Alexander Hinneb-\nurg. 2015. Exploring the space of topic coherence\nmeasures. In Proceedings of the Eighth ACM Interna-\ntional Conference on Web Search and Data Mining,\npages 399-408.\nDazhong Shen, Chuan Qin, Chao Wang, Zheng Dong,\nHengshu Zhu, and Hui Xiong. 2021. Topic modeling\nrevisited: A document graph-based neural network\nperspective. In Advances in Neural Information Pro-\ncessing Systems, volume 34, pages 14681-14693.\nCurran Associates, Inc.\nNino Shervashidze, Pascal Schweitzer, Erik Jan van\nLeeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt.\n2011. Weisfeiler-Lehman graph kernels. Journal of\nMachine Learning Research, 12(77):2539\u20132561.\nAkash Srivastava and Charles Sutton. 2017. Autoen-\ncoding variational inference for topic models. In\nProceedings of the 5th International Conference on\nLearning Representations, ICLR 2017.\nSilvia Terragni, Elisabetta Fersini, Bruno Giovanni\nGaluzzi, Pietro Tropeano, and Antonio Candelieri.\n2021a. OCTIS: Comparing and optimizing topic\nmodels is simple! In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: System Demonstra-\ntions, pages 263\u2013270. Association for Computational\nLinguistics.\nSilvia Terragni, Elisabetta Fersini, and Enza Messina.\n2021b. Word embedding-based topic similarity mea-\nsures. In Natural Language Processing and Informa-\ntion Systems, pages 33-45, Cham. Springer Interna-\ntional Publishing.\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li\u00f2, and Yoshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations.\nHanna Wallach, David Mimno, and Andrew McCallum.\n2009. Rethinking LDA: Why priors matter. In Ad-\nvances in Neural Information Processing Systems,\nvolume 22. Curran Associates, Inc.\nWilliam Webber, Alistair Moffat, and Justin Zobel. 2010.\nA similarity measure for indefinite rankings. ACM\nTransactions on Information Systems (TOIS), 28(4):1\u2013\n38.\nXiaobao Wu, Xinshuai Dong, Thong Nguyen, and\nAnh Tuan Luu. 2023. Effective neural topic mod-\neling with embedding clustering regularization. In\nProceedings of the 40th International Conference on\nMachine Learning, ICML'23.\nQianqian Xie, Jimin Huang, Pan Du, and Min Peng.\n2021. Graph relational topic model with higher-\norder graph attention auto-encoders. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 2604-2613, Online. Association\nfor Computational Linguistics.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie\nJegelka. 2019. How powerful are graph neural net-\nworks? In International Conference on Learning\nRepresentations.\nXiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi\nCheng. 2013. A biterm topic model for short texts.\nIn Proceedings of the 22nd International Conference\non World Wide Web, WWW '13, page 1445\u20131456,\nNew York, NY, USA. Association for Computing\nMachinery.\nLiang Yang, Fan Wu, Junhua Gu, Chuan Wang, Xi-\naochun Cao, Di Jin, and Yuanfang Guo. 2020. Graph\nattention topic modeling network. In Proceedings\nof The Web Conference 2020, WWW '20, page\n144-154, New York, NY, USA. Association for Com-\nputing Machinery.\nRenbo Zhao and Vincent Y. F. Tan. 2017. Online non-\nnegative matrix factorization with outliers. IEEE\nTransactions on Signal Processing, 65(3):555\u2013570.",
    "12": "Deyu Zhou, Xuemeng Hu, and Rui Wang. 2020. Neural\ntopic modeling by incorporating document relation-\nship graph. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 3790-3796, Online. Association\nfor Computational Linguistics.\nQile Zhu, Zheng Feng, and Xiaolin Li. 2018.\nGraphBTM: Graph enhanced autoencoded varia-\ntional inference for biterm topic model. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4663\u20134672,\nBrussels, Belgium. Association for Computational\nLinguistics.\nA Data Overview\nA.1 Dataset Descriptions\nDatasets used in experiments:\n1. 20NewsGroups (20NG) dataset comprising\n16, 309 pre-processed documents from 20 dif-\nferent newsgroups posts. Each document is\nlabeled with its corresponding category type.\n2. BBC News (BBC) (Greene and Cunningham,\n2006) dataset consists of 2, 225 news articles\nfrom BBC. Documents are categorized into\n5 different classes: tech, business, entertain-\nment, sports, and politics.\n3. SearchSnippets (SS) (Qiang et al., 2022) is\nderived from predefined phrases across 8 do-\nmains, this dataset is constructed from web\nsearch transactions. The domains include\nbusiness, computers, culture-arts, education-\nscience, engineering, health, politics-society,\nand sports.\n4. Biomedicine (Bio) (Qiang et al., 2022)\nmakes use of the challenge data delivered on\nBioASQ's official website.\n#No.\n2.\n3.\n4.\nLabel\n#Docs\n% Docs\n1.\nmisc.forsale\n861\n5.28\ncomp.windows.x\n883\n5.41\nsoc.religion.christian\n920\n5.64\ntalk.religion.misc\n521\n3.19\n5.\nrec.autos\n822\n5.04\n6.\nsci.med\n866\n5.31\n7.\ntalk.politics.misc\n689\n4.22\n8.\ntalk.politics.mideast\n828\n5.08\n9.\nsci.electronics\n867\n5.32\n10.\nrec.sport.hockey\n843\n5.17\n11.\nrec.sport.baseball\n787\n4.83\n12.\ntalk.politics.guns\n808\n4.95\n13.\nsci.crypt\n883\n5.41\n14.\ncomp.sys.mac.hardware\n838\n5.14\n15.\ncomp.sys.ibm.pc.hardware 891\n5.46\n16.\ncomp.graphics\n836\n5.13\n17.\ncomp.os.ms-windows.misc 828\n5.08\n18.\nalt.atheism\n689\n4.22\n19.\nsci.space\n856\n5.25\nrec.motorcycles\n793\n4.86\n20.\nTable 8: 20NG labels with corresponding document\ncounts and percentage of documents.\n#No. Label\n#Docs\n% Docs\n1.\ntech\n401\n18.02\n2.\nbusiness\n510\n22.92\n3.\nentertainment 386\n17.35\n4.\nsport\n511\n22.97\n5.\npolitics\n417\n18.74\nTable 9: BBC labels with corresponding document\ncounts and percentage of documents.\n#No.\nLabel\n#Docs\n% Docs\n5. StackOverflow (SO) (Qiang et al., 2022) The\ndataset is released on Kaggle.com. The raw\ndataset contains 3,370,528 samples from July\n31st, 2012 to August 14, 2012. Here, the\ndataset randomly selects 20,000 question titles\nfrom 20 different tags.\n1.\nbusiness\n2652\n21.61\n2.\ncomputers\n2177\n17.74\n3.\nculture-arts\n1499 12.22\n4.\neducation-science 1498\n3.01\n5.\nengineering\n1491 12.15\n6.\nhealth\n7.\npolitics-society 1173\n8.\nsports\n369\n1411 12.21\n9.56\n11.5\nThe initial two datasets, 20NG, and BBC, are\navailable on OCTIS\u00b2. As for the remaining three\ndatasets SS, Bio, and SO, we have pre-processed\nthem using the method detailed in Section A.2.\n2https://github.com/MIND-Lab/OCTIS\nTable 10: SS labels with corresponding document\ncounts and percentage of documents.",
    "13": "A.2 Preprocessing\nUsing OCTIS, we convert each document to lower-\ncase, remove the punctuations, lemmatize it, filter\nthe vocabulary with the most frequent 2000 terms,\nfilter words with less than 3 characters, and filter\ndocuments with less than 3 words.\nB Baseline Configurations\nWe reproduced all baseline models by following\nthe guidance provided in their original papers and\nutilizing codes from either the original sources\nor from OCTIS. Specifically, for CombinedTM\n(Bianchi et al., 2021a), ZeroShotTM (Bianchi\net al., 2021b), ProdLDA (Srivastava and Sutton,\n2017), NeuralLDA (Srivastava and Sutton, 2017),\nETM (Dieng et al., 2020), LDA (Blei et al., 2003),\nLSI (Dumais, 2004), NMF (Zhao and Tan, 2017),\nwe employed the implementation from OCTIS with\ndefault parameter values. For GraphBTM\u00b3 (Zhu\net al., 2018), GNTM4 (Shen et al., 2021), and\nECRTM5 (Wu et al., 2023) we utilized the offi-\ncial source codes. Hyperparameter optimization\nwas performed for GNTM on each dataset, and the\nvalues are detailed in Table 11. However, hyperpa-\nrameter optimization for GBTM is computationally\nintensive, likely due to its exhaustive consideration\nof all words in the vocabulary when constructing\nthe graph.\nHyperpramerts\nTemperature for STGS:\nWindow size for graph construction:\n3\n20NG BBC SS\n0.6 0.6 0.7\n10\n2\nTable 11: Hyperparameter values for GNTM on each\ndataset.\nC Coherence Metrics\nCoherence matrices are used to compute the rele-\nvance of the top words within topics. The NPMI\ntopic coherence for a given topic \u1e9ek with n top\nwords is calculated as follows:\nn\nn\n1\nNPMI (\u1e9ek)\n=\n\u03a3\u03a3\n(2)\ni=1 j=i+1\nlog\np(wi, wj)+e\np(wi)p(wj)\n- log (p(wi, w;) + \u20ac)\nHere, p(wi, w;) is the probability of co-\noccurrence of words w; and w; in a boolean sliding\nhttps://github.com/valdersoul/GraphBTM\nhttps://github.com/SmilesDZgk/GNTM\nShttps://github.com/BobXWu/ECRTM\nwindow in topic k, and p(w;) and p(w;) represent\nthe probability of the individual words' occurrence\nin topic k. e is a small positive constant to pre-\nvent zero in the log() function. NPMI ranges from\n-1 (words never co-occur) to +1 (they always co-\noccur). CV is computed using an indirect cosine\nmeasure along with NPMI scores over a boolean\nsliding window. In our experiments, we consider\nthe top 10 words for each topic (i.e., n = 10) to\ncompute NPMI and CV scores.\nD Diversity Metrics\nTopic diversity quantifies the uniqueness of gener-\nated topics. To measure the topic diversity we have\nused three following metrics: (i) IRBO (Bianchi\net al., 2021b), (ii) wI-M (Terragni et al., 2021b),\n(iii) wI-C (Terragni et al., 2021b). The IRBO gives\nO for identical topics and 1 for completely dissimi-\nlar topics. Suppose we are given a collection N of\nT topics where each topic is a list of words such\nthat the words at the beginning of the list have a\nhigher probability of occurrence (i.e., are more im-\nportant or more highly ranked) in the topic. Then,\nthe IRBO score of the topics is defined as,\nIRBO(N) = 1-\n\u03a3\u0395\u03a3- RBO(li,lj)\nn\nwhere n = (5) is the number of pairs of lists,\nand RBO(i, j) denotes the standard Rank-Biased\nOverlap between two ranked lists li and l; (Web-\nber et al., 2010). IRBO allows the comparison of\nlists that may not contain the same items, and in\nparticular, may not cover all items in the domain.\nTwo lists (topics) with overlapping words receive\na smaller IRBO score when the overlap occurs at\nthe highest ranks of the lists than when it occurs at\nlower ranks. IRBO is implemented in OCTIS.\nE Computing Infrastructure\nOur experiments were run on a workstation with\nIntel\u00ae Xeon\u00ae W-1350 @ 3.30GHz, 6 Cores, 12\nThreads, 16.0 GB RAM, NVIDIA RTX A4000\nGPU, CUDA Version: 12.2 and Ubuntu 22.04 op-\nerating system."
}