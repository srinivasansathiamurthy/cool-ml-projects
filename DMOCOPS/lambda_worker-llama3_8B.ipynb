{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec00b52a-f23e-490b-8c42-c07d7a3a30b9",
   "metadata": {},
   "source": [
    "## Design\n",
    "1. tasks it needs to run gets appended to a task queue (which it queries from s3 accordingly using the directory of the file)\n",
    "2. it processes tasks in batches of M. After each batch is complete, it sends \"OK\" to another json. It also updates its task queue from s3 (deletes these 20, and takes in more if more were added onto it)\n",
    "3. If it fails for any task, it sents \"NOT OK\" and the tasks on its taskqueue to s3 and script stops. Then batch_manager will kill that gpu and redistribute the tasks to an alive gpu.\n",
    "4. Once a gpu's task queue is empty, it dies.\n",
    "    - if the last gpu fails early, it'll tell batch manager that there were some tasks left over. then you just need to rerun those tasks when you wake up\n",
    "    - so the system won't be perfect, but it's risk free, and cost efficient, and it does make it a whole lot less annoying.\n",
    "    - also later on you can increase risk tolerance (say it only dies early if it fails some percentage of jobs, and failed jobs get added to some dead letter queue. But I'm not adding that in right now.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c299635-80ab-4c08-a727-d77ffedf6d79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (5.9.0)\n",
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in ./.local/lib/python3.10/site-packages (1.34.156)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.156 in ./.local/lib/python3.10/site-packages (from boto3) (1.34.156)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.local/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in ./.local/lib/python3.10/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.local/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.156->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.35.0,>=1.34.156->boto3) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.156->boto3) (1.16.0)\n",
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.44.0)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.0.1)\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.10/site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in ./.local/lib/python3.10/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/lib/python3/dist-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.10/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.10/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2020.6.20)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.33.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil\n",
    "!pip install boto3\n",
    "!pip install transformers torch huggingface_hub\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667654ac-a5d1-4391-bbd0-e71ed9faccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8edb27-8568-41c5-9fae-f2fa334e6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid.\n"
     ]
    }
   ],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = 'key1'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'key2'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef2d4b-3000-452b-9ad3-b61659b487fe",
   "metadata": {},
   "source": [
    "## Inject Model for Inference\n",
    "\n",
    "- this is just for llm ops, not multimodal ops, so just a simple inference function should be ok\n",
    "- Super modular (just inject your inference code here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df693bf-fa8e-4b27-bb9a-56ae599270cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid token passed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Log in to Hugging Face\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model_id = \"meta-llama/Meta-Llama-3-8B\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/_login.py:111\u001b[0m, in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe token has not been saved to the git credentials helper. Pass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_to_git_credential=True` in this function directly or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--add-to-git-credential` if using via `huggingface-cli` if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou want to set the git credential as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[0;32m--> 111\u001b[0m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_permission\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_permission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[1;32m    113\u001b[0m     notebook_login(new_session\u001b[38;5;241m=\u001b[39mnew_session, write_permission\u001b[38;5;241m=\u001b[39mwrite_permission)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/_login.py:307\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential, write_permission)\u001b[0m\n\u001b[1;32m    305\u001b[0m permission \u001b[38;5;241m=\u001b[39m get_token_permission(token)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m permission \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid token passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m write_permission \u001b[38;5;129;01mand\u001b[39;00m permission \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is valid but is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m token is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide a new token with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m correct permission.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid token passed!"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"hf_key\")\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de1a67a3-3265-4e83-904a-44bb1bd82264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=2.0 \n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output_ids[0])\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e2ae86b-6272-47cd-8700-9ad84edb1aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>hi i have a question about the \"magnetic field\" and also\\nthe magnetic force exerted by it on an object.\\ni am reading my textbook but its not clear to me, what is this thing called \\'MAGNETIC FIELD\\'???\\nand how does one calculate or determine whether there exists such kind of forces in any given situation? (like if two magnets are near each other)????\\n\\n## Answers\\n\\n    - Magnetic Field: A region around magnet where all objects experience'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e1173-e830-4704-bb70-a86fad59b3e9",
   "metadata": {},
   "source": [
    "## The Worker \n",
    "\n",
    "-  cluster_num and bucket_name need to be inputted. Everything else is good as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "126ff311-63b6-4d15-8d02-efb9514caa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_to_dict(bucket_name, object_name):\n",
    "    try:\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_name)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        data_dict = json.loads(content)\n",
    "        # print(f'File {object_name} downloaded from bucket {bucket_name} and converted to dictionary.')\n",
    "        return data_dict\n",
    "    except (NoCredentialsError, PartialCredentialsError):\n",
    "        print('Credentials not available.')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "        return None\n",
    "\n",
    "def upload_dict_to_s3(bucket_name, object_name, dictionary):\n",
    "    json_data = json.dumps(dictionary)\n",
    "    s3.put_object(Bucket=bucket_name, Key=object_name, Body=json_data)\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    try:\n",
    "        if region:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                # CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        else:\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "        print(f'Bucket {bucket_name} created successfully.')\n",
    "    except s3.exceptions.BucketAlreadyExists:\n",
    "        print(f'Bucket {bucket_name} already exists.')\n",
    "    except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f'Bucket {bucket_name} is already owned by you.')\n",
    "    except (NoCredentialsError, PartialCredentialsError):\n",
    "        print('Credentials not available.')\n",
    "\n",
    "region = 'us-east-1'\n",
    "cluster_num = 0 #hardcode which batch_num this gpu is associated with.\n",
    "bucket_name = f\"rapper-vkg-{cluster_num}\"\n",
    "process_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ef902d5-dab7-415e-8f4e-b31f79289e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "063a9bf4-da53-4630-88e0-f565a44a83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskqueue_dict = download_file_to_dict(bucket_name, f\"task_queue_{cluster_num}.json\")\n",
    "taskqueue = taskqueue_dict[\"task_queue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dff32a8-a5aa-4759-b45e-181ca01094df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket rapper-vkg-0-results created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_bucket(f\"{bucket_name}-results\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae72898a-3fb1-427b-88cc-266b4044feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just one prompt. p ez to refactor to multiprompt.\n",
    "prompt = \"Is this politically correct? If so, explain why?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a585cef-d359-40ce-b31a-20f0c320fc85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_3.json\n",
      "data_3.json has been completed\n",
      "data_10.json\n",
      "data_10.json has been completed\n",
      "data_17.json\n",
      "data_17.json has been completed\n",
      "data_24.json\n",
      "data_24.json has been completed\n",
      "data_31.json\n",
      "data_31.json has been completed\n",
      "data_38.json\n",
      "data_38.json has been completed\n",
      "data_45.json\n",
      "data_45.json has been completed\n",
      "data_52.json\n",
      "data_52.json has been completed\n",
      "data_59.json\n",
      "data_59.json has been completed\n",
      "data_66.json\n",
      "data_66.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  58\n",
      "data_465.json\n",
      "data_465.json has been completed\n",
      "data_150.json\n",
      "data_150.json has been completed\n",
      "data_381.json\n",
      "data_381.json has been completed\n",
      "data_472.json\n",
      "data_472.json has been completed\n",
      "data_325.json\n",
      "data_325.json has been completed\n",
      "data_353.json\n",
      "data_353.json has been completed\n",
      "data_241.json\n",
      "data_241.json has been completed\n",
      "data_122.json\n",
      "data_122.json has been completed\n",
      "data_283.json\n",
      "data_283.json has been completed\n",
      "data_262.json\n",
      "data_262.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  58\n",
      "data_324.json\n",
      "data_324.json has been completed\n",
      "data_198.json\n",
      "data_198.json has been completed\n",
      "data_296.json\n",
      "data_296.json has been completed\n",
      "data_220.json\n",
      "data_220.json has been completed\n",
      "data_339.json\n",
      "data_339.json has been completed\n",
      "data_135.json\n",
      "data_135.json has been completed\n",
      "data_408.json\n",
      "data_408.json has been completed\n",
      "data_101.json\n",
      "data_101.json has been completed\n",
      "data_94.json\n",
      "data_94.json has been completed\n",
      "data_451.json\n",
      "data_451.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  48\n",
      "data_129.json\n",
      "data_129.json has been completed\n",
      "data_444.json\n",
      "data_444.json has been completed\n",
      "data_423.json\n",
      "data_423.json has been completed\n",
      "data_275.json\n",
      "data_275.json has been completed\n",
      "data_331.json\n",
      "data_331.json has been completed\n",
      "data_115.json\n",
      "data_115.json has been completed\n",
      "data_458.json\n",
      "data_458.json has been completed\n",
      "data_136.json\n",
      "data_136.json has been completed\n",
      "data_227.json\n",
      "data_227.json has been completed\n",
      "data_430.json\n",
      "data_430.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  48\n",
      "data_477.json\n",
      "data_477.json has been completed\n",
      "data_183.json\n",
      "data_183.json has been completed\n",
      "data_162.json\n",
      "data_162.json has been completed\n",
      "data_323.json\n",
      "data_323.json has been completed\n",
      "data_219.json\n",
      "data_219.json has been completed\n",
      "data_402.json\n",
      "data_402.json has been completed\n",
      "data_157.json\n",
      "data_157.json has been completed\n",
      "data_234.json\n",
      "data_234.json has been completed\n",
      "data_199.json\n",
      "data_199.json has been completed\n",
      "data_297.json\n",
      "data_297.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  38\n",
      "data_346.json\n",
      "data_346.json has been completed\n",
      "data_374.json\n",
      "data_374.json has been completed\n",
      "data_143.json\n",
      "data_143.json has been completed\n",
      "data_388.json\n",
      "data_388.json has been completed\n",
      "data_311.json\n",
      "data_311.json has been completed\n",
      "data_416.json\n",
      "data_416.json has been completed\n",
      "data_332.json\n",
      "data_332.json has been completed\n",
      "data_318.json\n",
      "data_318.json has been completed\n",
      "data_276.json\n",
      "data_276.json has been completed\n",
      "data_80.json\n",
      "data_80.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  48\n",
      "data_294.json\n",
      "data_294.json has been completed\n",
      "data_449.json\n",
      "data_449.json has been completed\n",
      "data_364.json\n",
      "data_364.json has been completed\n",
      "data_478.json\n",
      "data_478.json has been completed\n",
      "data_310.json\n",
      "data_310.json has been completed\n",
      "data_252.json\n",
      "data_252.json has been completed\n",
      "data_346.json\n",
      "data_346.json has been completed\n",
      "data_374.json\n",
      "data_374.json has been completed\n",
      "data_143.json\n",
      "data_143.json has been completed\n",
      "data_388.json\n",
      "data_388.json has been completed\n",
      "A batch of size 10 has been completed\n",
      "Length of taskqueue:  38\n",
      "data_105.json\n",
      "data_105.json has been completed\n",
      "data_175.json\n",
      "data_175.json has been completed\n",
      "data_311.json\n",
      "data_311.json has been completed\n",
      "data_245.json\n",
      "data_245.json has been completed\n",
      "data_416.json\n",
      "data_416.json has been completed\n",
      "data_332.json\n"
     ]
    }
   ],
   "source": [
    "while(taskqueue):\n",
    "    try:\n",
    "        for i in range(min(process_batch_size, len(taskqueue))):\n",
    "            file = taskqueue[i]\n",
    "            print(file)\n",
    "            tf = download_file_to_dict(bucket_name, file)\n",
    "            context = tf[\"text\"]\n",
    "            full_prompt = f\"\"\"\n",
    "            CONTEXT: \n",
    "            {context}\n",
    "\n",
    "            PROMPT: \n",
    "            {prompt}\n",
    "            \"\"\"\n",
    "            tf.update(\n",
    "                {\n",
    "                    \"model_inference\": run_model(full_prompt)\n",
    "                }\n",
    "            )\n",
    "            upload_dict_to_s3(f\"{bucket_name}-results\", f\"{file}_results.json\", tf)\n",
    "            print(f\"{file} has been completed\")\n",
    "        taskqueue = taskqueue[process_batch_size:]\n",
    "        temp_taskqueue = download_file_to_dict(bucket_name, f\"task_queue_{cluster_num}.json\")[\"task_queue\"]\n",
    "        taskqueue = list(set(taskqueue)|set(temp_taskqueue[process_batch_size:]))\n",
    "        upload_dict_to_s3(bucket_name, f\"task_queue_{cluster_num}.json\", {\"task_queue\": taskqueue})\n",
    "        print(f\"A batch of size {process_batch_size} has been completed\")\n",
    "        print(\"Length of taskqueue: \", len(taskqueue))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"WORKER FAILED\")\n",
    "        taskqueue_dict[\"task_queue\"] = taskqueue\n",
    "        taskqueue_dict[\"status\"] = \"NOT OK\"\n",
    "        upload_dict_to_s3(bucket_name, f\"task_queue_{cluster_num}.json\", taskqueue_dict)\n",
    "        print(\"updated task queue\")\n",
    "        break\n",
    "\n",
    "print(\"exited process\")\n",
    "if taskqueue_dict.get(\"status\", None) != \"NOT OK\":\n",
    "    print(\"updating status to complete\")\n",
    "    taskqueue_dict[\"status\"] = \"COMPLETE\"\n",
    "    upload_dict_to_s3(bucket_name, f\"task_queue_{cluster_num}.json\", taskqueue_dict)\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
