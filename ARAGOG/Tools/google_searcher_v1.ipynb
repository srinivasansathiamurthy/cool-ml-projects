{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d59dc42-7ca2-42c7-92f9-ced8d045eff3",
   "metadata": {},
   "source": [
    "# What is this\n",
    "\n",
    "Allows you to interact with google search and extract top K links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94241da-5199-4e38-9711-a6791fda44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a30a3315-d0b8-4763-8da2-82801ebe9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the query:  alaibvaiuerbgancaperngf\n",
      "Enter the number of top URLs to fetch:  1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1\n",
      "Fetching from: https://www.google.com/search?q=alaibvaiuerbgancaperngf#ip=1\n",
      "Attempt 2\n",
      "No more pages found.\n",
      "Found 43 links for 'alaibvaiuerbgancaperngf'\n"
     ]
    }
   ],
   "source": [
    "def initialize_driver():\n",
    "    options = Options()\n",
    "    # options.add_argument('--headless=new') #comment this out if you wanna debug visually\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def get_search_query_url(query):\n",
    "    base_url = 'https://www.google.com/search?q='\n",
    "    query_string = query.replace(\" \", \"%2B\")\n",
    "    return f\"{base_url}{query_string}#ip=1\"\n",
    "\n",
    "def scroll_to_bottom(driver):\n",
    "    last_scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_scroll_height == last_scroll_height:\n",
    "            break\n",
    "        last_scroll_height = new_scroll_height\n",
    "\n",
    "def extract_links_from_page(driver, search_term):\n",
    "    page_source = driver.page_source\n",
    "    hrefs = driver.find_elements(By.PARTIAL_LINK_TEXT, search_term)\n",
    "    links = [element.get_attribute(\"href\") for element in hrefs]\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    pattern = re.compile(r'<a\\s+jsname=\"UWckNb\"\\s+href=\"([^\"]+)\"')\n",
    "    matches = re.findall(pattern, page_source)\n",
    "    links.extend(matches)\n",
    "    \n",
    "    return links\n",
    "    \n",
    "def fetch_links_for_query(query, top_n, search_term = ''):\n",
    "    all_links = []\n",
    "    driver = initialize_driver()\n",
    "\n",
    "    current_url = \"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while len(all_links) < top_n:\n",
    "        print(f\"Attempt {attempt + 1}\")\n",
    "        attempt += 1\n",
    "\n",
    "        if attempt == 1:\n",
    "            query_url = get_search_query_url(query)\n",
    "            driver.get(query_url)\n",
    "        else:\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, '//a[@id=\"pnnext\"]')\n",
    "                next_button.click()\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Wait for the page to load and ensure it is different from the previous one\n",
    "                while driver.current_url == current_url:\n",
    "                    time.sleep(1)\n",
    "            except Exception:\n",
    "                print(\"No more pages found.\")\n",
    "                break\n",
    "\n",
    "        current_url = driver.current_url\n",
    "        print(f\"Fetching from: {current_url}\")\n",
    "\n",
    "        scroll_to_bottom(driver)\n",
    "        links = extract_links_from_page(driver, search_term)\n",
    "        all_links.extend(links)\n",
    "\n",
    "        if len(all_links) >= top_n:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return all_links[:top_n]  # Return only the top N links\n",
    "\n",
    "\n",
    "# Prompt user for query and number of URLs\n",
    "query = input(\"Enter the query: \")\n",
    "top_n = int(input(\"Enter the number of top URLs to fetch: \"))\n",
    "## not super effective. yer better off getting all the urls and then searching through them via regex/etc.\n",
    "# search_term = input(\"Enter text you want contained in url (optional): \") \n",
    "links = fetch_links_for_query(query, top_n)\n",
    "\n",
    "print(f\"Found {len(links)} links for '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc5e7f5-3b5a-4921-8bb7-b31702f82100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 links for 'alaibvaiuerbgancaperngf'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(links)} links for '{query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0e6d7-fd3e-4ce7-861c-628b47d555b8",
   "metadata": {},
   "source": [
    "## observations/ extensions:\n",
    "\n",
    "1. If no subfilter, then it links to a bunch of interesting things, like related google searchers, google maps, google images, flights (?), etc. Also some \"None\"s are returned.\n",
    "2. I could look for \"tail links\". Make a scrappy google indexer via a bunch of random queries, try and categorize them and group by occurence (soft grouping)...\n",
    "    - probably need to do this. maybe not at the model level, but def at the eda level.\n",
    "  \n",
    "## but what does this do rn:\n",
    "1. given a query, it searches through all pages and gets links in a list. good enough for basic thing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
